

Google search: size of gpt-3

GPT-3's full version has a capacity of 175 billion machine learning parameters. 

[GPT-3](https://en.wikipedia.org/wiki/GPT-3#:~:text=GPT%2D3's%20full%20version%20has,of%20pre%2Dtrained%20language%20representations.), Wikipedia

People also ask
How many GB is GPT-3?
* According to the OpenAI's whitepaper, GPT-3 uses half-precision floating-point variables at 16 bits per parameter. 
* This means the model would require at least 350 GB of VRAM just to load the model and run inference at a decent speed. 
* This is the equivalent of at least 11 Tesla V100 GPUs with 32 GB of memory each.
[The GPT-3 economy – TechTalks](https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/)


Google search: half-precision floating point

In computing, half precision (sometimes called FP16) is a binary floating-point computer number format that occupies 16 bits (two bytes in modern computers) in computer memory.

[Half-precision floating-point format](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)

Google search: gpt-3 how many gpus?

In order to train GPT-3, Microsoft spent money to save a supercomputer. 
In May of this year, Microsoft officially announced the launch of one of the world's top five supercomputers, specifically for OpenAI model training. 
It has 285,000 CPU cores and 10,000 Nvidia V100 GPUs.

* [Burning 10,000 GPUs from Microsoft, this group is determined to smash everyone’s jobs](http://www.ww01.net/en/archives/85465#:~:text=In%20order%20to%20train%20GPT,and%2010%2C000%20Nvidia%20V100%20GPUs.)

How many layers does GPT-3 have?
96 layers
GPT-3 has 96 layers with each layer having 96 attention heads. 
Size of word embeddings was increased to 12888 for GPT-3 from 1600 for GPT-2. 
Context window size was increased from 1024 for GPT-2 to 2048 tokens for GPT-3.

[The Journey of Open AI GPT models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2), Walmart Global Tech
